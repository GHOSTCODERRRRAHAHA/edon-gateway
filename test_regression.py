"""
Regression tests for production safety invariants.

These tests ensure critical production safety features never regress:
- No traceback leakage
- HTTPException status codes preserved (especially 503)
- No file paths in error messages

NOTE:
- Clawdbot integration tests MUST use test-only credential_ids and MUST pass credential_id
  into /clawdbot/invoke so we never clobber the real/dev default credential.
- Restoring the original Clawdbot integration is best-effort and only attempted when
  a real secret is available via env var.
"""

import os
import json
import requests
import pytest

BASE_URL = os.getenv("EDON_GATEWAY_URL", "http://localhost:8000").rstrip("/")

AUTH_TOKEN = os.getenv("EDON_API_TOKEN", "test-token")
AUTH_ENABLED = os.getenv("EDON_AUTH_ENABLED", "").strip().lower() == "true"

# Optional: If you want restore to be real, set these in your env/CI
# (EDON cannot recover secrets from status endpoints)
CLAWDBOT_GATEWAY_URL = os.getenv("CLAWDBOT_GATEWAY_URL", "http://127.0.0.1:18789").rstrip("/")
CLAWDBOT_GATEWAY_SECRET = os.getenv("CLAWDBOT_GATEWAY_SECRET")  # must be real to restore
CLAWDBOT_GATEWAY_AUTH_MODE = os.getenv("CLAWDBOT_GATEWAY_AUTH_MODE", "token")

# Dedicated test-only credential_ids (never collide with your real one)
TEST_CRED_WRONG_TOKEN = os.getenv("EDON_TEST_CLAWDBOT_CRED_ID", "clawdbot_gateway_wrong_token_test")
TEST_CRED_503 = os.getenv("EDON_TEST_CLAWDBOT_CRED_ID_503", "clawdbot_gateway_downstream_503_test")

# Dedicated tenant for regression tests
TEST_TENANT_ID = os.getenv("EDON_TEST_TENANT_ID", "tenant_dev")


def _assert_auth_ready():
    if not AUTH_ENABLED:
        raise SystemExit("EDON_AUTH_ENABLED is not true. Set EDON_AUTH_ENABLED=true and EDON_API_TOKEN.")
    if not AUTH_TOKEN or AUTH_TOKEN == "test-token":
        raise SystemExit("EDON_API_TOKEN is missing/placeholder. Set EDON_API_TOKEN to a valid token.")


def edon_headers(extra=None):
    """Headers for EDON Gateway requests. Includes X-EDON-TOKEN when auth enabled and X-Tenant-ID always."""
    h = {"Content-Type": "application/json", "X-Tenant-ID": TEST_TENANT_ID}
    if AUTH_ENABLED and AUTH_TOKEN:
        h["X-EDON-TOKEN"] = AUTH_TOKEN
    if extra:
        # do not allow accidental removal of tenant header
        h.update(extra)
        if "X-Tenant-ID" not in h:
            h["X-Tenant-ID"] = TEST_TENANT_ID
    return h


def _get_current_clawdbot_integration_status():
    """Best-effort: fetch current Clawdbot integration status (does NOT include secret)."""
    try:
        r = requests.get(
            f"{BASE_URL}/integrations/account/integrations",
            headers=edon_headers(),
            timeout=10,
        )
        if r.status_code != 200:
            return {}
        data = r.json() or {}
        return data.get("clawdbot") or {}
    except Exception:
        return {}


def _restore_clawdbot_integration_best_effort(saved_status):
    """
    Best-effort restore. This only restores if:
    - saved_status contains a base_url/auth_mode
    - and we have a real secret via env var (CLAWDBOT_GATEWAY_SECRET)
    """
    base_url = (saved_status or {}).get("base_url")
    auth_mode = (saved_status or {}).get("auth_mode") or CLAWDBOT_GATEWAY_AUTH_MODE

    if not base_url:
        return
    if not CLAWDBOT_GATEWAY_SECRET:
        return

    try:
        requests.post(
            f"{BASE_URL}/integrations/clawdbot/connect",
            json={
                "base_url": base_url,
                "auth_mode": auth_mode,
                "secret": CLAWDBOT_GATEWAY_SECRET,
                "probe": False,
            },
            headers=edon_headers(),
            timeout=10,
        )
    except Exception:
        pass


def _apply_pack(agent_id: str):
    """Apply policy pack and return intent_id or skip."""
    r = requests.post(
        f"{BASE_URL}/policy-packs/clawdbot_safe/apply",
        json={},
        headers=edon_headers(extra={"X-Agent-ID": agent_id}),
        timeout=10,
    )
    if r.status_code != 200:
        return ("skip", f"apply returned {r.status_code}: {r.text[:200]}")
    intent_id = (r.json() or {}).get("intent_id")
    if not intent_id:
        return ("skip", "Apply response missing intent_id")
    return intent_id


def _connect_test_credential(*, credential_id: str, base_url: str, secret: str, auth_mode: str = "token"):
    """Connect a test-only credential."""
    return requests.post(
        f"{BASE_URL}/integrations/clawdbot/connect",
        json={
            "credential_id": credential_id,
            "base_url": base_url,
            "auth_mode": auth_mode,
            "secret": secret,
            "probe": False,
        },
        headers=edon_headers(),
        timeout=10,
    )


def _invoke_with_cred(*, intent_id: str, agent_id: str, credential_id: str, tool: str, args=None):
    """Invoke Clawdbot tool via EDON, explicitly specifying credential_id."""
    return requests.post(
        f"{BASE_URL}/clawdbot/invoke",
        headers=edon_headers(extra={"X-Intent-ID": intent_id, "X-Agent-ID": agent_id}),
        json={
            "credential_id": credential_id,
            "tool": tool,
            "action": "json",
            "args": args or {},
        },
        timeout=15,
    )


def test_no_traceback_leakage():
    print("Testing: No traceback leakage...")

    response = requests.post(
        f"{BASE_URL}/execute",
        json={
            "action": {
                "tool": "email",
                "op": "send",
                "params": {
                    "recipients": ["test@example.com"],
                    "subject": "Test",
                    "body": "Test",
                },
            },
            "agent_id": "test-agent-001",
        },
        headers=edon_headers(),
        timeout=15,
    )

    response_text = response.text.lower()
    assert "traceback" not in response_text, f"Response contains 'traceback': {response.text}"
    assert 'file "' not in response_text, f"Response contains file path: {response.text}"
    assert "c:\\" not in response_text, f"Response contains Windows path: {response.text}"
    if "detail" in response_text:
        prefix = response_text.split("detail", 1)[0]
        assert "line " not in prefix, f"Response contains line numbers: {response.text}"
    else:
        assert "line " not in response_text[:200], f"Response contains line numbers: {response.text}"

    print(f"  [OK] No traceback leakage (status: {response.status_code})")


def test_clawdbot_invoke_persists_decision_or_audit():
    print("Testing: Clawdbot invoke persists decision/audit...")

    intent_id = _apply_pack("regression-test-agent")
    if isinstance(intent_id, tuple):
        pytest.skip(intent_id[1])

    # Use the real/dev credential for this one (itâ€™s a positive-path regression)
    invoke_resp = requests.post(
        f"{BASE_URL}/clawdbot/invoke",
        headers=edon_headers(extra={"X-Agent-ID": "regression-test-agent", "X-Intent-ID": intent_id}),
        json={"tool": "sessions_list", "action": "json", "args": {}},
        timeout=15,
    )
    assert invoke_resp.status_code == 200, f"Invoke failed: {invoke_resp.status_code} {invoke_resp.text}"
    invoke_data = invoke_resp.json() or {}
    assert "edon_verdict" in invoke_data, f"Invoke response missing edon_verdict: {invoke_data}"

    dec_resp = requests.get(
        f"{BASE_URL}/decisions/query",
        params={"intent_id": intent_id, "limit": 10},
        headers=edon_headers(),
        timeout=10,
    )
    assert dec_resp.status_code == 200, f"Decisions query failed: {dec_resp.status_code}"
    dec_data = dec_resp.json() or {}
    decisions_total = dec_data.get("total", len(dec_data.get("decisions", [])))

    audit_resp = requests.get(
        f"{BASE_URL}/audit/query",
        params={"intent_id": intent_id, "limit": 10},
        headers=edon_headers(),
        timeout=10,
    )
    assert audit_resp.status_code == 200, f"Audit query failed: {audit_resp.status_code}"
    audit_data = audit_resp.json() or {}
    audit_total = audit_data.get("total", len(audit_data.get("events", [])))

    assert decisions_total > 0 or audit_total > 0, (
        f"Expected at least one decision or audit record for intent_id={intent_id}; "
        f"got decisions total={decisions_total}, audit total={audit_total}"
    )
    print(f"  [OK] Clawdbot invoke persistence: decisions={decisions_total}, audit={audit_total}")


def test_clawdbot_wrong_token_401_clean_error():
    """
    Wrong Clawdbot secret must yield a clean 401 path.

    Invariant:
    - HTTP 401 (status code), JSON response, no traceback / file paths.
    """
    print("Testing: Clawdbot wrong token yields clean 401 error path...")

    saved = _get_current_clawdbot_integration_status()

    intent_id = _apply_pack("regression-test-agent")
    if isinstance(intent_id, tuple):
        pytest.skip(intent_id[1])

    connect_resp = _connect_test_credential(
        credential_id=TEST_CRED_WRONG_TOKEN,
        base_url=CLAWDBOT_GATEWAY_URL,
        secret="wrong_secret_never_valid",
        auth_mode="token",
    )
    assert connect_resp.status_code == 200, f"connect failed: {connect_resp.status_code} {connect_resp.text}"
    connect_json = connect_resp.json() or {}
    used_credential_id = connect_json.get("credential_id") or TEST_CRED_WRONG_TOKEN

    try:
        invoke_resp = _invoke_with_cred(
            intent_id=intent_id,
            agent_id="regression-test-agent",
            credential_id=used_credential_id,
            tool="sessions_list",
            args={},
        )

        assert invoke_resp.headers.get("content-type", "").startswith("application/json"), (
            f"Expected JSON response, got {invoke_resp.headers.get('content-type')}"
        )
        assert invoke_resp.status_code == 401, (
            f"Expected 401, got {invoke_resp.status_code}: {invoke_resp.text}"
        )

        data = invoke_resp.json() or {}
        blob = json.dumps(data).lower()
        assert "traceback" not in blob, f"Traceback leaked in error: {data}"
        assert 'file "' not in blob and "c:\\" not in blob and "/users/" not in blob and "/home/" not in blob, (
            f"File path leaked in error: {data}"
        )

        print("  [OK] Wrong token yields clean 401 error (no traceback)")
    finally:
        _restore_clawdbot_integration_best_effort(saved)


def test_clawdbot_sessions_smoke():
    print("Testing: Clawdbot sessions_get, sessions_create, sessions_update smoke...")

    intent_id = _apply_pack("regression-sessions-smoke")
    if isinstance(intent_id, tuple):
        pytest.skip(intent_id[1])

    # Positive-path smoke uses default/dev credential
    h = edon_headers(extra={"X-Agent-ID": "regression-sessions-smoke", "X-Intent-ID": intent_id})

    def invoke(tool, args=None):
        r = requests.post(
            f"{BASE_URL}/clawdbot/invoke",
            headers=h,
            json={"tool": tool, "action": "json", "args": args or {}},
            timeout=15,
        )
        try:
            body = r.json()
        except Exception:
            body = {}
        return (r.status_code == 200 and body.get("ok") is True), r

    ok_list, r_list = invoke("sessions_list")
    if not ok_list:
        pytest.skip(f"sessions_list failed (Clawdbot may be down): {r_list.text[:200]}")

    ok_get, r_get = invoke("sessions_get", {"sessionKey": "main"})
    assert r_get.status_code == 200, f"sessions_get failed: {r_get.status_code} {r_get.text}"

    ok_create, r_create = invoke("sessions_create", {})
    assert r_create.status_code == 200, f"sessions_create failed: {r_create.status_code} {r_create.text}"

    ok_update, r_update = invoke("sessions_update", {"sessionKey": "main"})
    assert r_update.status_code == 200, f"sessions_update failed: {r_update.status_code} {r_update.text}"

    print("  [OK] sessions_get, sessions_create, sessions_update smoke")


def test_503_preserved():
    """
    Test that ALLOW + downstream unavailable yields HTTP 503.
    Deterministic: uses a dedicated dead credential_id and invokes with that credential_id.
    """
    print("Testing: 503 status code preservation (ALLOW + downstream unavailable -> 503)...")

    saved = _get_current_clawdbot_integration_status()

    intent_id = _apply_pack("test-agent-503")
    if isinstance(intent_id, tuple):
        pytest.skip(intent_id[1])

    dead_url = "http://127.0.0.1:1"

    connect_resp = _connect_test_credential(
        credential_id=TEST_CRED_503,
        base_url=dead_url,
        secret="irrelevant",
        auth_mode="token",
    )
    assert connect_resp.status_code == 200, f"connect failed: {connect_resp.status_code} {connect_resp.text}"
    connect_json = connect_resp.json() or {}
    used_credential_id = connect_json.get("credential_id") or TEST_CRED_503

    # Optional sanity print
    check_resp = requests.get(
        f"{BASE_URL}/integrations/account/integrations",
        headers=edon_headers(),
        timeout=10,
    )
    print("DEBUG integrations/account/integrations status:", check_resp.status_code)
    try:
        check_json = check_resp.json()
    except Exception:
        check_json = {}
    print("DEBUG clawdbot integration after connect:", json.dumps((check_json or {}).get("clawdbot"), indent=2))

    try:
        print("DEBUG invoking with tenant:", TEST_TENANT_ID)

        response = _invoke_with_cred(
            intent_id=intent_id,
            agent_id="test-agent-503",
            credential_id=used_credential_id,
            tool="sessions_list",
            args={},
        )

        assert response.status_code == 503, (
            f"Expected 503 (ALLOW + downstream unavailable), got {response.status_code}: {response.text}"
        )
        print("  [OK] 503 status code preserved (ALLOW + downstream unavailable -> 503)")
    finally:
        _restore_clawdbot_integration_best_effort(saved)


def test_no_file_paths_in_errors():
    print("Testing: No file paths in error messages...")

    test_cases = [
        {"action": {"tool": "invalid_tool", "op": "test"}, "agent_id": "test"},
        {"action": {}, "agent_id": "test"},
    ]

    for test_case in test_cases:
        try:
            response = requests.post(
                f"{BASE_URL}/execute",
                json=test_case,
                headers=edon_headers(),
                timeout=15,
            )
            response_text = response.text.lower()

            assert "c:\\" not in response_text, f"Response contains Windows path: {response.text}"
            assert "/users/" not in response_text and "/home/" not in response_text, (
                f"Response contains Unix path: {response.text}"
            )

            tail = response_text.split("detail")[-1][:200]
            assert ".py" not in tail, f"Response contains Python file reference: {response.text}"
        except Exception:
            pass

    print("  [OK] No file paths in error messages")


def test_error_envelope_consistency():
    print("Testing: Error envelope consistency...")

    error_scenarios = [
        (lambda: requests.post(
            f"{BASE_URL}/execute",
            json={"action": {}, "agent_id": "test"},
            headers=edon_headers(),
            timeout=10,
        ), None),
        (lambda: requests.post(
            f"{BASE_URL}/execute",
            json={
                "action": {"tool": "email", "op": "draft", "params": {"body": "<script>alert(1)</script>"}},
                "agent_id": "test",
            },
            headers=edon_headers(),
            timeout=10,
        ), None),
    ]

    for make_request, _expected_status in error_scenarios:
        try:
            response = make_request()

            ct = response.headers.get("content-type", "")
            if not ct.startswith("application/json"):
                if response.status_code >= 400:
                    assert False, f"Error response should be JSON. status={response.status_code} ct={ct} body={response.text[:200]}"
                continue

            data = response.json()
            if response.status_code >= 400:
                assert "detail" in data, f"Error response should have 'detail' field: {data}"
                assert isinstance(data["detail"], str), f"Error detail should be string: {data}"
        except Exception:
            pass

    print("  [OK] Error envelope consistency")


def run_regression_tests():
    _assert_auth_ready()

    print("=" * 70)
    print("Production Safety Regression Tests")
    print("=" * 70)
    print()

    tests = [
        ("No Traceback Leakage", test_no_traceback_leakage),
        ("Clawdbot invoke persists decision/audit", test_clawdbot_invoke_persists_decision_or_audit),
        ("Clawdbot wrong token 401 clean error", test_clawdbot_wrong_token_401_clean_error),
        ("Clawdbot sessions smoke (get/create/update)", test_clawdbot_sessions_smoke),
        ("503 Status Preserved", test_503_preserved),
        ("No File Paths in Errors", test_no_file_paths_in_errors),
        ("Error Envelope Consistency", test_error_envelope_consistency),
    ]

    results = {"passed": 0, "failed": 0, "skipped": 0, "errors": [], "skipped_reasons": []}

    for test_name, test_func in tests:
        try:
            print(f"\n{test_name}:")
            out = test_func()
            if isinstance(out, tuple) and len(out) == 2 and out[0] == "skip":
                results["skipped"] += 1
                results["skipped_reasons"].append(f"{test_name}: {out[1]}")
            else:
                results["passed"] += 1
        except pytest.SkipTest as e:
            results["skipped"] += 1
            results["skipped_reasons"].append(f"{test_name}: {e}")
        except AssertionError as e:
            print(f"  [FAIL] FAILED: {str(e)}")
            results["failed"] += 1
            results["errors"].append(f"{test_name}: {str(e)}")
        except Exception as e:
            print(f"  [FAIL] ERROR: {str(e)}")
            results["failed"] += 1
            results["errors"].append(f"{test_name}: {str(e)}")

    print("\n" + "=" * 70)
    print("Regression Test Summary")
    print("=" * 70)
    print(f"Passed: {results['passed']}")
    print(f"Failed: {results['failed']}")
    print(f"Skipped: {results['skipped']}")
    print(f"Total: {results['passed'] + results['failed'] + results['skipped']}")

    if results["errors"]:
        print("\nErrors:")
        for error in results["errors"]:
            print(f"  - {error}")

    if results["skipped_reasons"]:
        print("\nSkipped reasons:")
        for reason in results["skipped_reasons"]:
            print(f"  - {reason}")

    if results["failed"] > 0 or results["skipped"] > 0:
        print("\n[FAIL] Some regression tests failed or were skipped!")
        return 1

    print("\n[OK] All regression tests passed!")
    return 0


if __name__ == "__main__":
    import sys
    sys.exit(run_regression_tests())
